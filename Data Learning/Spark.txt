Apache Spark is an open-source, distributed processing system designed for big data workloads. 
It processes data in parallel across a cluster of computers, providing scalability and high performance. 
Spark supports various programming languages like Python, Scala, Java, and R.

The Plan is evaluated lazily.

Driver :
Driver Reads the Plan 
Important Spark Driver Settings:
    1. spark.driver.memory : Specifies the amount of memory allocated to the driver process. 
    The driver is responsible for orchestrating tasks, scheduling, and maintaining metadata about the job.
    2. spark.driver.memoryOverheadFactor : Defines the additional memory overhead allocated to the driver for non-JVM processes,
     such as Python or R (in PySpark or SparkR) or memory required for off-heap storage.

Driver needs to determine a few things:
    1. When to actually start executing the job and stop being lazy.
    2. How to join datasets
    3. How much parallelism each step needs.

Executors: WHo actual do the work 
    the Driver passes the plan to the executor:
        spark.executor.memory
        spark.executor.cores  : how many task can happen on each machine (default 4 should not go higher than 6)
        spark.executor.memoryOverheadFactor

The types of joins in spark:
    1. Shuffle sort merge join:
        - Default JOin strategy since Spark 2.3
        - Works when both sides of the join are large
    2. Broadcast Hash join 
        - Works well if the left side of the join is small
        - sparl.sql.autobroadcastjointhreshold (default is 10 MBs can go as high as 8GB, you will experience weird memory problems > 1GBs)
        - A join without Shuffle 
    3. Bucket Join 
        - A join without Shuffle
        - Both datasets must have the same number of buckets for the bucket join to work efficiently.

Shuffle:
Shuffle Partition and Parallelism are linked:
    - spark.sql.Shuffle.Partition and spark.default.parallelism

Is Shuffle good or bad?
    - At low to medium volumn
        - Its really good and makes our lives easier
    - At high volumes > 10TBs is painful

Shuffle and skew:
Sometimes some partitions have dramatically more data than others, This can happen because:
    - Not enough partitions 

Check data is skewed:
    - more scientific way is to do a box and whiskers plot of the data 

Ways to deal with skew:
    - Adaptive query execution - only in Spark 3+
        - Set spark.sql.adaptive.enabled = True
    - Salting the group by - best option before Spark 3 
        - Group by a random number, aggregate + group by again

How to look at Spark query plans:
    - Use explain() on your dataframes : show join strategies