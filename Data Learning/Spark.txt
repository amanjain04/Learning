Apache Spark is an open-source, distributed processing system designed for big data workloads. 
It processes data in parallel across a cluster of computers, providing scalability and high performance. 
Spark supports various programming languages like Python, Scala, Java, and R.

The Plan is evaluated lazily.

Driver :
Driver Reads the Plan 
Important Spark Driver Settings:
    1. spark.driver.memory : Specifies the amount of memory allocated to the driver process. 
    The driver is responsible for orchestrating tasks, scheduling, and maintaining metadata about the job.
    2. spark.driver.memoryOverheadFactor : Defines the additional memory overhead allocated to the driver for non-JVM processes,
     such as Python or R (in PySpark or SparkR) or memory required for off-heap storage.

Driver needs to determine a few things:
    1. When to actually start executing the job and stop being lazy.
    2. How to join datasets
    3. How much parallelism each step needs.

Executors: WHo actual do the work 
    the Driver passes the plan to the executor:
        spark.executor.memory
        spark.executor.cores  : how many task can happen on each machine (default 4 should not go higher than 6)
        spark.executor.memoryOverheadFactor

The types of joins in spark:
    1. Shuffle sort merge join:
        - Default JOin strategy since Spark 2.3
        - Works when both sides of the join are large
    2. Broadcast Hash join 
        - Works well if the left side of the join is small
        - sparl.sql.autobroadcastjointhreshold (default is 10 MBs can go as high as 8GB, you will experience weird memory problems > 1GBs)
        - A join without Shuffle 
    3. Bucket Join 
        - A join without Shuffle